{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI Neural Net Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "currDir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "rootDir = os.path.abspath(os.path.join(currDir, '..'))\n",
    "sys.path.insert(1, rootDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from src.data.make_dataset import make_dataset_1, make_dataset_2\n",
    "from src.utils.file_utils import save_df, save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate artificial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully generated dataset | num_rows: 2600000\n",
      "successfully generated dataset | num_rows: 504000\n",
      "df successfully saved | filename: dataset1.csv, dir: /Users/test/Documents/GitHub/xai-senior-design/data\n",
      "df successfully saved | filename: dataset2.csv, dir: /Users/test/Documents/GitHub/xai-senior-design/data\n"
     ]
    }
   ],
   "source": [
    "df1 = make_dataset_1()\n",
    "df2 = make_dataset_2()\n",
    "save_df(df1, \"dataset1.csv\")\n",
    "save_df(df2, \"dataset2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>ei</th>\n",
       "      <th>to</th>\n",
       "      <th>td</th>\n",
       "      <th>tf</th>\n",
       "      <th>vers</th>\n",
       "      <th>consumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.160000e+01</td>\n",
       "      <td>1.510916e+00</td>\n",
       "      <td>2.940000e+01</td>\n",
       "      <td>6.505000e+01</td>\n",
       "      <td>1.050000e+01</td>\n",
       "      <td>4.500000e+00</td>\n",
       "      <td>1.484044e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.002501e+00</td>\n",
       "      <td>1.132716e+00</td>\n",
       "      <td>3.891838e+01</td>\n",
       "      <td>3.752776e+01</td>\n",
       "      <td>5.766282e+00</td>\n",
       "      <td>2.872282e+00</td>\n",
       "      <td>3.282216e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.899000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.479167e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>9.330000e-01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.257500e+01</td>\n",
       "      <td>5.750000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>6.713139e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.500000e+00</td>\n",
       "      <td>1.354955e+00</td>\n",
       "      <td>4.500000e+00</td>\n",
       "      <td>6.505000e+01</td>\n",
       "      <td>1.050000e+01</td>\n",
       "      <td>4.500000e+00</td>\n",
       "      <td>3.509333e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.100000e+01</td>\n",
       "      <td>1.354955e+00</td>\n",
       "      <td>5.500000e+01</td>\n",
       "      <td>9.752500e+01</td>\n",
       "      <td>1.525000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>1.428353e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.600000e+01</td>\n",
       "      <td>4.814286e+00</td>\n",
       "      <td>1.200000e+02</td>\n",
       "      <td>1.300000e+02</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>6.292000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mode            ei            to            td            tf  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   1.160000e+01  1.510916e+00  2.940000e+01  6.505000e+01  1.050000e+01   \n",
       "std    8.002501e+00  1.132716e+00  3.891838e+01  3.752776e+01  5.766282e+00   \n",
       "min    5.000000e+00  5.899000e-01  1.000000e+00  1.000000e-01  1.000000e+00   \n",
       "25%    5.000000e+00  9.330000e-01  3.000000e+00  3.257500e+01  5.750000e+00   \n",
       "50%    6.500000e+00  1.354955e+00  4.500000e+00  6.505000e+01  1.050000e+01   \n",
       "75%    2.100000e+01  1.354955e+00  5.500000e+01  9.752500e+01  1.525000e+01   \n",
       "max    2.600000e+01  4.814286e+00  1.200000e+02  1.300000e+02  2.000000e+01   \n",
       "\n",
       "               vers   consumption  \n",
       "count  2.600000e+06  2.600000e+06  \n",
       "mean   4.500000e+00  1.484044e+02  \n",
       "std    2.872282e+00  3.282216e+02  \n",
       "min    0.000000e+00  6.479167e-06  \n",
       "25%    2.000000e+00  6.713139e+00  \n",
       "50%    4.500000e+00  3.509333e+01  \n",
       "75%    7.000000e+00  1.428353e+02  \n",
       "max    9.000000e+00  6.292000e+03  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.file_utils import load_df\n",
    "df1, df2 = load_df(\"dataset1.csv\"), load_df(\"dataset2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model():    \n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(6, input_dim=5, activation=\"relu\"))#hidden layer\n",
    "#     model.add(Dense(10, activation=\"relu\"))#hidden layer\n",
    "#     model.add(Dense(1, activation='sigmoid'))#output layer\n",
    "\n",
    "#     optimizer = RMSprop(0.001)\n",
    "#     model.compile(loss='mse', optimizer=optimizer, metrics=['mse', 'mae', 'mape'])\n",
    "#     return model\n",
    "\n",
    "def build_model(num_features, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_shape=(num_features,), activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', # Cross-entropy\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df successfully saved | filename: norm_test2.csv, dir: /Users/test/Documents/GitHub/xai-senior-design/data\n"
     ]
    }
   ],
   "source": [
    "def prepare_df(df, y_column):\n",
    "    temp_df = df.copy()\n",
    "    temp_df.sample(frac=1)\n",
    "    \n",
    "    y = to_categorical(\n",
    "        temp_df[y_column].values)\n",
    "    temp_df.drop(y_column, axis=1, inplace=True)\n",
    "    features = list(temp_df.columns)\n",
    "    \n",
    "    x = temp_df.values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x)\n",
    "    x = scaler.transform(x)\n",
    "    \n",
    "    return x,y,features\n",
    "\n",
    "x1, y1, features1 = prepare_df(df1, \"vers\")\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size=0.20)\n",
    "norm_train1, norm_test1 = pd.DataFrame(x_train1, columns=features1), pd.DataFrame(x_test1, columns=features1)\n",
    "save_df(norm_train1, \"norm_train1.csv\")\n",
    "save_df(norm_train1, \"norm_test1.csv\")\n",
    "\n",
    "x2, y2, features2 = prepare_df(df2, \"vers\")\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.20)\n",
    "norm_train2, norm_test2 = pd.DataFrame(x_train2, columns=features2), pd.DataFrame(x_test2, columns=features2)\n",
    "save_df(norm_train2, \"norm_train2.csv\")\n",
    "save_df(norm_train2, \"norm_test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1664000 samples, validate on 416000 samples\n",
      "Epoch 1/10\n",
      "1664000/1664000 - 29s - loss: 1.1614 - accuracy: 0.5902 - val_loss: 0.7835 - val_accuracy: 0.7200\n",
      "Epoch 2/10\n",
      "1664000/1664000 - 27s - loss: 0.6724 - accuracy: 0.7616 - val_loss: 0.5938 - val_accuracy: 0.7868\n",
      "Epoch 3/10\n",
      "1664000/1664000 - 29s - loss: 0.5573 - accuracy: 0.7978 - val_loss: 0.5280 - val_accuracy: 0.8013\n",
      "Epoch 4/10\n",
      "1664000/1664000 - 27s - loss: 0.5017 - accuracy: 0.8145 - val_loss: 0.4876 - val_accuracy: 0.8228\n",
      "Epoch 5/10\n",
      "1664000/1664000 - 29s - loss: 0.4639 - accuracy: 0.8260 - val_loss: 0.4491 - val_accuracy: 0.8238\n",
      "Epoch 6/10\n",
      "1664000/1664000 - 28s - loss: 0.4364 - accuracy: 0.8339 - val_loss: 0.4355 - val_accuracy: 0.8340\n",
      "Epoch 7/10\n",
      "1664000/1664000 - 26s - loss: 0.4148 - accuracy: 0.8402 - val_loss: 0.4253 - val_accuracy: 0.8381\n",
      "Epoch 8/10\n",
      "1664000/1664000 - 23s - loss: 0.3979 - accuracy: 0.8459 - val_loss: 0.4107 - val_accuracy: 0.8379\n",
      "Epoch 9/10\n",
      "1664000/1664000 - 23s - loss: 0.3836 - accuracy: 0.8503 - val_loss: 0.3758 - val_accuracy: 0.8594\n",
      "Epoch 10/10\n",
      "1664000/1664000 - 23s - loss: 0.3715 - accuracy: 0.8544 - val_loss: 0.3614 - val_accuracy: 0.8645\n"
     ]
    }
   ],
   "source": [
    "model1 = build_model(6,10)\n",
    "history1 = model1.fit(x_train1, y_train1, epochs=10, validation_split=0.2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 322560 samples, validate on 80640 samples\n",
      "Epoch 1/15\n",
      "322560/322560 - 4s - loss: 1.3205 - accuracy: 0.4360 - val_loss: 1.0828 - val_accuracy: 0.5200\n",
      "Epoch 2/15\n",
      "322560/322560 - 5s - loss: 0.9931 - accuracy: 0.5446 - val_loss: 0.9260 - val_accuracy: 0.6175\n",
      "Epoch 3/15\n",
      "322560/322560 - 4s - loss: 0.8885 - accuracy: 0.5886 - val_loss: 0.8630 - val_accuracy: 0.5528\n",
      "Epoch 4/15\n",
      "322560/322560 - 5s - loss: 0.8285 - accuracy: 0.6200 - val_loss: 0.7966 - val_accuracy: 0.6636\n",
      "Epoch 5/15\n",
      "322560/322560 - 5s - loss: 0.7851 - accuracy: 0.6387 - val_loss: 0.7638 - val_accuracy: 0.6720\n",
      "Epoch 6/15\n",
      "322560/322560 - 6s - loss: 0.7514 - accuracy: 0.6559 - val_loss: 0.7330 - val_accuracy: 0.6476\n",
      "Epoch 7/15\n",
      "322560/322560 - 5s - loss: 0.7249 - accuracy: 0.6625 - val_loss: 0.7109 - val_accuracy: 0.6681\n",
      "Epoch 8/15\n",
      "322560/322560 - 5s - loss: 0.7024 - accuracy: 0.6749 - val_loss: 0.6779 - val_accuracy: 0.6990\n",
      "Epoch 9/15\n",
      "322560/322560 - 4s - loss: 0.6786 - accuracy: 0.6972 - val_loss: 0.7025 - val_accuracy: 0.6615\n",
      "Epoch 10/15\n",
      "322560/322560 - 4s - loss: 0.6541 - accuracy: 0.7221 - val_loss: 0.6848 - val_accuracy: 0.7276\n",
      "Epoch 11/15\n",
      "322560/322560 - 4s - loss: 0.6224 - accuracy: 0.7471 - val_loss: 0.5968 - val_accuracy: 0.7947\n",
      "Epoch 12/15\n",
      "322560/322560 - 5s - loss: 0.5873 - accuracy: 0.7814 - val_loss: 0.5607 - val_accuracy: 0.8306\n",
      "Epoch 13/15\n",
      "322560/322560 - 5s - loss: 0.5506 - accuracy: 0.8067 - val_loss: 0.5273 - val_accuracy: 0.8199\n",
      "Epoch 14/15\n",
      "322560/322560 - 4s - loss: 0.5185 - accuracy: 0.8252 - val_loss: 0.5194 - val_accuracy: 0.8199\n",
      "Epoch 15/15\n",
      "322560/322560 - 4s - loss: 0.4921 - accuracy: 0.8379 - val_loss: 0.4674 - val_accuracy: 0.8605\n"
     ]
    }
   ],
   "source": [
    "model2 = build_model(5,7)\n",
    "history = model2.fit(x_train2, y_train2, epochs=20, validation_split=0.2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520000/520000 [==============================] - 17s 33us/sample - loss: 0.3584 - accuracy: 0.8658\n",
      "[0.3583500788943126, 0.8658192]\n",
      "100800/100800 [==============================] - 3s 33us/sample - loss: 0.4695 - accuracy: 0.8580\n",
      "[0.4694996028239765, 0.8580159]\n"
     ]
    }
   ],
   "source": [
    "print(model1.evaluate(x_test1, y_test1))\n",
    "print(model2.evaluate(x_test2, y_test2))\n",
    "# [0.09895923781607832, 0.85723215]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n",
      "4 4\n",
      "7 2\n",
      "4 4\n",
      "9 8\n",
      "2 2\n",
      "4 4\n",
      "6 6\n",
      "5 9\n",
      "0 2\n"
     ]
    }
   ],
   "source": [
    "preds = model2.predict_classes(x_test2)\n",
    "# print(preds)\n",
    "for i in range(len(y_test2[:10])):\n",
    "    print(preds[i], list(y_test2[i]).index(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model successfully saved | file_location: /Users/test/Documents/GitHub/xai-senior-design/models/model1.h5\n",
      "model successfully saved | file_location: /Users/test/Documents/GitHub/xai-senior-design/models/model2.h5\n"
     ]
    }
   ],
   "source": [
    "save_model(model1, \"model1.h5\")\n",
    "save_model(model2, \"model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
