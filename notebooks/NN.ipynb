{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI Neural Net Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "currDir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "rootDir = os.path.abspath(os.path.join(currDir, '..'))\n",
    "sys.path.insert(1, rootDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from src.data.make_dataset import make_dataset_1, make_dataset_2\n",
    "from src.utils.file_utils import save_df, save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate artificial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully generated dataset | num_rows: 2600000\n",
      "successfully generated dataset | num_rows: 504000\n",
      "df successfully saved | filename: dataset1.csv, dir: C:\\Users\\archg\\school\\senoir\\xai-senior-design\\data\n",
      "df successfully saved | filename: dataset2.csv, dir: C:\\Users\\archg\\school\\senoir\\xai-senior-design\\data\n"
     ]
    }
   ],
   "source": [
    "df1 = make_dataset_1()\n",
    "df2 = make_dataset_2()\n",
    "save_df(df1, \"dataset1.csv\")\n",
    "save_df(df2, \"dataset2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>ei</th>\n",
       "      <th>to</th>\n",
       "      <th>td</th>\n",
       "      <th>tf</th>\n",
       "      <th>vers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.160000e+01</td>\n",
       "      <td>1.854442e+00</td>\n",
       "      <td>2.677224e+02</td>\n",
       "      <td>7.155500e+01</td>\n",
       "      <td>1.216666e+01</td>\n",
       "      <td>4.500000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.002501e+00</td>\n",
       "      <td>3.186181e+00</td>\n",
       "      <td>1.555065e+03</td>\n",
       "      <td>4.702838e+01</td>\n",
       "      <td>1.125711e+01</td>\n",
       "      <td>2.872282e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>3.479820e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>9.330000e-01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.430000e+01</td>\n",
       "      <td>3.968246e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.500000e+00</td>\n",
       "      <td>1.354955e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>6.850000e+01</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>4.500000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.100000e+01</td>\n",
       "      <td>1.354955e+00</td>\n",
       "      <td>5.500000e+01</td>\n",
       "      <td>1.027000e+02</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.600000e+01</td>\n",
       "      <td>2.317735e+01</td>\n",
       "      <td>1.440000e+04</td>\n",
       "      <td>2.600000e+02</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>9.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mode            ei            to            td            tf  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   1.160000e+01  1.854442e+00  2.677224e+02  7.155500e+01  1.216666e+01   \n",
       "std    8.002501e+00  3.186181e+00  1.555065e+03  4.702838e+01  1.125711e+01   \n",
       "min    5.000000e+00  3.479820e-01  1.000000e+00  1.000000e-01  1.000000e+00   \n",
       "25%    5.000000e+00  9.330000e-01  3.000000e+00  3.430000e+01  3.968246e+00   \n",
       "50%    6.500000e+00  1.354955e+00  5.000000e+00  6.850000e+01  9.000000e+00   \n",
       "75%    2.100000e+01  1.354955e+00  5.500000e+01  1.027000e+02  1.700000e+01   \n",
       "max    2.600000e+01  2.317735e+01  1.440000e+04  2.600000e+02  6.000000e+01   \n",
       "\n",
       "               vers  \n",
       "count  2.600000e+06  \n",
       "mean   4.500000e+00  \n",
       "std    2.872282e+00  \n",
       "min    0.000000e+00  \n",
       "25%    2.000000e+00  \n",
       "50%    4.500000e+00  \n",
       "75%    7.000000e+00  \n",
       "max    9.000000e+00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.file_utils import load_df\n",
    "df1, df2 = load_df(\"dataset1.csv\"), load_df(\"dataset2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model():    \n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(6, input_dim=5, activation=\"relu\"))#hidden layer\n",
    "#     model.add(Dense(10, activation=\"relu\"))#hidden layer\n",
    "#     model.add(Dense(1, activation='sigmoid'))#output layer\n",
    "\n",
    "#     optimizer = RMSprop(0.001)\n",
    "#     model.compile(loss='mse', optimizer=optimizer, metrics=['mse', 'mae', 'mape'])\n",
    "#     return model\n",
    "\n",
    "def build_model(num_features, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(num_features,), activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', # Cross-entropy\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_model2(num_features, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, input_shape=(num_features,), activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', # Cross-entropy\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df successfully saved | filename: norm_train1.csv, dir: C:\\Users\\archg\\school\\senoir\\xai-senior-design\\data\n",
      "df successfully saved | filename: norm_test1.csv, dir: C:\\Users\\archg\\school\\senoir\\xai-senior-design\\data\n",
      "df successfully saved | filename: norm_train2.csv, dir: C:\\Users\\archg\\school\\senoir\\xai-senior-design\\data\n",
      "df successfully saved | filename: norm_test2.csv, dir: C:\\Users\\archg\\school\\senoir\\xai-senior-design\\data\n"
     ]
    }
   ],
   "source": [
    "def prepare_df(df, y_column):\n",
    "    temp_df = df.copy()\n",
    "    temp_df.sample(frac=1)\n",
    "    \n",
    "    y = to_categorical(\n",
    "        temp_df[y_column].values)\n",
    "    temp_df.drop(y_column, axis=1, inplace=True)\n",
    "    features = list(temp_df.columns)\n",
    "    \n",
    "    x = temp_df.values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x)\n",
    "    x = scaler.transform(x)\n",
    "    \n",
    "    u = scaler.mean_\n",
    "    s = scaler.scale_\n",
    "    \n",
    "    return x,y,features,u,s\n",
    "\n",
    "x1, y1, features1,u1,s1 = prepare_df(df1, \"vers\")\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size=0.20)\n",
    "norm_train1, norm_test1 = pd.DataFrame(x_train1, columns=features1), pd.DataFrame(x_test1, columns=features1)\n",
    "\n",
    "\n",
    "#Only save the train-test if they need to be updated. Be sure to also train the NN and save\n",
    "\n",
    "save_df(norm_train1, \"norm_train1.csv\")\n",
    "save_df(norm_test1, \"norm_test1.csv\")\n",
    "\n",
    "x2, y2, features2,u2,s2 = prepare_df(df2, \"vers\")\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.20)\n",
    "norm_train2, norm_test2 = pd.DataFrame(x_train2, columns=features2), pd.DataFrame(x_test2, columns=features2)\n",
    "save_df(norm_train2, \"norm_train2.csv\")\n",
    "save_df(norm_test2, \"norm_test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mode', 'ei', 'to', 'td', 'tf']\n",
      "means1:\n",
      "[ 11.6          1.85444168 267.72244306  71.555       12.16665978]\n",
      "std1:\n",
      "[   8.00249961    3.18618066 1555.06491616   47.02836883   11.25710397]\n",
      "['mode', 'speed', 'fe', 'tt']\n",
      "means2:\n",
      "[  2.5         65.2602466    0.95246209 729.52536085]\n",
      "std2:\n",
      "[1.70782513e+00 1.92803439e+02 3.08688256e-01 2.32262065e+03]\n"
     ]
    }
   ],
   "source": [
    "print(features1)\n",
    "print(\"means1:\")\n",
    "print(u1)\n",
    "print(\"std1:\")\n",
    "print(s1)\n",
    "\n",
    "print(features2)\n",
    "print(\"means2:\")\n",
    "print(u2)\n",
    "print(\"std2:\")\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1664000 samples, validate on 416000 samples\n",
      "Epoch 1/20\n",
      "1664000/1664000 - 28s - loss: 0.6232 - accuracy: 0.7485 - val_loss: 0.5101 - val_accuracy: 0.7915\n",
      "Epoch 2/20\n",
      "1664000/1664000 - 27s - loss: 0.5135 - accuracy: 0.7812 - val_loss: 0.4790 - val_accuracy: 0.7739\n",
      "Epoch 3/20\n",
      "1664000/1664000 - 26s - loss: 0.4708 - accuracy: 0.7875 - val_loss: 0.4627 - val_accuracy: 0.7929\n",
      "Epoch 4/20\n",
      "1664000/1664000 - 27s - loss: 0.4527 - accuracy: 0.7918 - val_loss: 0.4369 - val_accuracy: 0.7968\n",
      "Epoch 5/20\n",
      "1664000/1664000 - 26s - loss: 0.4416 - accuracy: 0.7943 - val_loss: 0.4223 - val_accuracy: 0.8041\n",
      "Epoch 6/20\n",
      "1664000/1664000 - 26s - loss: 0.4329 - accuracy: 0.7960 - val_loss: 0.4820 - val_accuracy: 0.7583\n",
      "Epoch 7/20\n",
      "1664000/1664000 - 26s - loss: 0.4256 - accuracy: 0.7978 - val_loss: 0.4173 - val_accuracy: 0.7958\n",
      "Epoch 8/20\n",
      "1664000/1664000 - 26s - loss: 0.4127 - accuracy: 0.8001 - val_loss: 0.4005 - val_accuracy: 0.8026\n",
      "Epoch 9/20\n",
      "1664000/1664000 - 26s - loss: 0.4085 - accuracy: 0.8006 - val_loss: 0.3786 - val_accuracy: 0.8173\n",
      "Epoch 10/20\n",
      "1664000/1664000 - 26s - loss: 0.4052 - accuracy: 0.8020 - val_loss: 0.3859 - val_accuracy: 0.8129\n",
      "Epoch 11/20\n",
      "1664000/1664000 - 27s - loss: 0.4028 - accuracy: 0.8026 - val_loss: 0.3679 - val_accuracy: 0.8137\n",
      "Epoch 12/20\n",
      "1664000/1664000 - 27s - loss: 0.3973 - accuracy: 0.8051 - val_loss: 0.4170 - val_accuracy: 0.7918\n",
      "Epoch 13/20\n",
      "1664000/1664000 - 26s - loss: 0.3995 - accuracy: 0.8037 - val_loss: 0.3685 - val_accuracy: 0.8190\n",
      "Epoch 14/20\n",
      "1664000/1664000 - 27s - loss: 0.3958 - accuracy: 0.8049 - val_loss: 0.3724 - val_accuracy: 0.8121\n",
      "Epoch 15/20\n",
      "1664000/1664000 - 27s - loss: 0.3933 - accuracy: 0.8055 - val_loss: 0.3682 - val_accuracy: 0.8161\n",
      "Epoch 16/20\n",
      "1664000/1664000 - 26s - loss: 0.3892 - accuracy: 0.8075 - val_loss: 0.3762 - val_accuracy: 0.8141\n",
      "Epoch 17/20\n",
      "1664000/1664000 - 26s - loss: 0.3886 - accuracy: 0.8073 - val_loss: 0.3878 - val_accuracy: 0.8174\n",
      "Epoch 18/20\n",
      "1664000/1664000 - 27s - loss: 0.3868 - accuracy: 0.8077 - val_loss: 0.4401 - val_accuracy: 0.7866\n",
      "Epoch 19/20\n",
      "1664000/1664000 - 27s - loss: 0.3835 - accuracy: 0.8092 - val_loss: 0.3595 - val_accuracy: 0.8192\n",
      "Epoch 20/20\n",
      "1664000/1664000 - 26s - loss: 0.3785 - accuracy: 0.8107 - val_loss: 0.3522 - val_accuracy: 0.8187\n"
     ]
    }
   ],
   "source": [
    "model1 = build_model(5,10)\n",
    "history1 = model1.fit(x_train1, y_train1, epochs=20, validation_split=0.2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520000/520000 [==============================] - 13s 25us/sample - loss: 0.3519 - accuracy: 0.8182\n",
      "[0.35193492987201763, 0.81815386]\n"
     ]
    }
   ],
   "source": [
    "print(model1.evaluate(x_test1, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.29990629 -0.33495956 -0.13679329  0.66438622  0.6958575 ] 0 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17151853  0.09877017 -0.28130324] 9 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17023241  0.59846856 -0.72546721] 3 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17151853  1.05563942  0.6958575 ] 9 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17023241 -0.08409818 -0.54780162] 0 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.16958935 -0.21168074  0.34052632] 0 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17072366  0.24974287 -0.9031328 ] 0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[-0.82474231 -0.2166909  -0.17087547 -1.00481903 -0.54780162] 8 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17151853 -1.46836902  0.25169353] 9 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 0.29990629 -0.38753542 -0.13679329 -1.28975343 -0.8143    ] 7 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[ 1.17463298 -0.34097006 -0.12393209 -1.44497889 -0.8143    ] 8 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17151853 -0.76028578 -0.63663441] 9 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[ 1.17463298 -0.39688323 -0.12393209  0.72179837 -0.72546721] 0 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[ 1.17463298 -0.4728105  -0.12393209  0.21784723 -0.99196559] 7 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[ 1.17463298 -0.39688323 -0.12393209 -0.92189036 -0.99196559] 0 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17151853  0.51766626  0.42935912] 9 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0.44985944 -0.2367856  -0.17023241  1.22149676  0.25169353] 8 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17087547  0.37945182  0.25169353] 9 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.16958935 -1.26636329 -0.28130324] 0 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17023241  0.36881994 -0.01480485] 0 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0.44985944 -0.2022615  -0.16637405  0.69840823 -0.01480485] 1 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17087547  0.56232016  0.25169353] 9 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17087547 -0.86235183 -0.63663441] 9 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[-0.82474231 -0.15676661 -0.17151853 -0.85384633  0.51819191] 9 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "a = model1.predict_classes(x_test1[:100])\n",
    "b = y_test1[:100]\n",
    "\n",
    "for idx, val in enumerate(a):\n",
    "    if b[idx][val] == 0:\n",
    "        print(x_test1[idx], a[idx],b[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1664000 samples, validate on 416000 samples\n",
      "Epoch 1/10\n",
      "1664000/1664000 - 15s - loss: 1.4354 - accuracy: 0.4673 - val_loss: 1.1788 - val_accuracy: 0.5643\n",
      "Epoch 2/10\n",
      "1664000/1664000 - 14s - loss: 1.0991 - accuracy: 0.6031 - val_loss: 1.0573 - val_accuracy: 0.6286\n",
      "Epoch 3/10\n",
      "1664000/1664000 - 14s - loss: 1.0345 - accuracy: 0.6335 - val_loss: 1.0144 - val_accuracy: 0.6443\n",
      "Epoch 4/10\n",
      "1664000/1664000 - 14s - loss: 0.9969 - accuracy: 0.6462 - val_loss: 0.9842 - val_accuracy: 0.6501\n",
      "Epoch 5/10\n",
      "1664000/1664000 - 14s - loss: 0.9660 - accuracy: 0.6513 - val_loss: 0.9557 - val_accuracy: 0.6552\n",
      "Epoch 6/10\n",
      "1664000/1664000 - 14s - loss: 0.9427 - accuracy: 0.6554 - val_loss: 0.9376 - val_accuracy: 0.6523\n",
      "Epoch 7/10\n",
      "1664000/1664000 - 14s - loss: 0.9265 - accuracy: 0.6586 - val_loss: 0.9192 - val_accuracy: 0.6683\n",
      "Epoch 8/10\n",
      "1664000/1664000 - 14s - loss: 0.9155 - accuracy: 0.6626 - val_loss: 0.9131 - val_accuracy: 0.6582\n",
      "Epoch 9/10\n",
      "1664000/1664000 - 15s - loss: 0.9063 - accuracy: 0.6673 - val_loss: 0.9021 - val_accuracy: 0.6633\n",
      "Epoch 10/10\n",
      "1664000/1664000 - 14s - loss: 0.8999 - accuracy: 0.6706 - val_loss: 0.8962 - val_accuracy: 0.6699\n"
     ]
    }
   ],
   "source": [
    "model1_2 = build_model2(5,10)\n",
    "history1_2 = model1_2.fit(x_train1, y_train1, epochs=10, validation_split=0.2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 322560 samples, validate on 80640 samples\n",
      "Epoch 1/15\n",
      "322560/322560 - 5s - loss: 0.2283 - accuracy: 0.9166 - val_loss: 0.1406 - val_accuracy: 0.9609\n",
      "Epoch 2/15\n",
      "322560/322560 - 5s - loss: 0.1545 - accuracy: 0.9473 - val_loss: 0.1396 - val_accuracy: 0.9550\n",
      "Epoch 3/15\n",
      "322560/322560 - 5s - loss: 0.1470 - accuracy: 0.9496 - val_loss: 0.1336 - val_accuracy: 0.9559\n",
      "Epoch 4/15\n",
      "322560/322560 - 6s - loss: 0.1419 - accuracy: 0.9531 - val_loss: 0.1147 - val_accuracy: 0.9670\n",
      "Epoch 5/15\n",
      "322560/322560 - 5s - loss: 0.1407 - accuracy: 0.9531 - val_loss: 0.1155 - val_accuracy: 0.9677\n",
      "Epoch 6/15\n",
      "322560/322560 - 5s - loss: 0.1409 - accuracy: 0.9534 - val_loss: 0.1202 - val_accuracy: 0.9645\n",
      "Epoch 7/15\n",
      "322560/322560 - 5s - loss: 0.1370 - accuracy: 0.9548 - val_loss: 0.1245 - val_accuracy: 0.9624\n",
      "Epoch 8/15\n",
      "322560/322560 - 5s - loss: 0.1323 - accuracy: 0.9570 - val_loss: 0.1320 - val_accuracy: 0.9601\n",
      "Epoch 9/15\n",
      "322560/322560 - 5s - loss: 0.1323 - accuracy: 0.9573 - val_loss: 0.1197 - val_accuracy: 0.9663\n",
      "Epoch 10/15\n",
      "322560/322560 - 5s - loss: 0.1299 - accuracy: 0.9587 - val_loss: 0.1324 - val_accuracy: 0.9572\n",
      "Epoch 11/15\n",
      "322560/322560 - 6s - loss: 0.1346 - accuracy: 0.9575 - val_loss: 0.1610 - val_accuracy: 0.9415\n",
      "Epoch 12/15\n",
      "322560/322560 - 6s - loss: 0.1308 - accuracy: 0.9586 - val_loss: 0.1482 - val_accuracy: 0.9503\n",
      "Epoch 13/15\n",
      "322560/322560 - 5s - loss: 0.1257 - accuracy: 0.9606 - val_loss: 0.1638 - val_accuracy: 0.9440\n",
      "Epoch 14/15\n",
      "322560/322560 - 5s - loss: 0.1302 - accuracy: 0.9590 - val_loss: 0.1174 - val_accuracy: 0.9635\n",
      "Epoch 15/15\n",
      "322560/322560 - 5s - loss: 0.1264 - accuracy: 0.9601 - val_loss: 0.1077 - val_accuracy: 0.9698\n"
     ]
    }
   ],
   "source": [
    "model2 = build_model(4,7)\n",
    "history2 = model2.fit(x_train2, y_train2, epochs=15, validation_split=0.2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 322560 samples, validate on 80640 samples\n",
      "Epoch 1/15\n",
      "322560/322560 - 3s - loss: 1.2289 - accuracy: 0.5297 - val_loss: 0.8262 - val_accuracy: 0.6320\n",
      "Epoch 2/15\n",
      "322560/322560 - 3s - loss: 0.6525 - accuracy: 0.7405 - val_loss: 0.5253 - val_accuracy: 0.8259\n",
      "Epoch 3/15\n",
      "322560/322560 - 3s - loss: 0.4474 - accuracy: 0.8823 - val_loss: 0.3788 - val_accuracy: 0.9137\n",
      "Epoch 4/15\n",
      "322560/322560 - 3s - loss: 0.3311 - accuracy: 0.9189 - val_loss: 0.2885 - val_accuracy: 0.9445\n",
      "Epoch 5/15\n",
      "322560/322560 - 3s - loss: 0.2608 - accuracy: 0.9423 - val_loss: 0.2367 - val_accuracy: 0.9439\n",
      "Epoch 6/15\n",
      "322560/322560 - 3s - loss: 0.2218 - accuracy: 0.9452 - val_loss: 0.2073 - val_accuracy: 0.9492\n",
      "Epoch 7/15\n",
      "322560/322560 - 3s - loss: 0.1993 - accuracy: 0.9480 - val_loss: 0.1920 - val_accuracy: 0.9602\n",
      "Epoch 8/15\n",
      "322560/322560 - 3s - loss: 0.1847 - accuracy: 0.9498 - val_loss: 0.1808 - val_accuracy: 0.9608\n",
      "Epoch 9/15\n",
      "322560/322560 - 3s - loss: 0.1750 - accuracy: 0.9510 - val_loss: 0.1706 - val_accuracy: 0.9589\n",
      "Epoch 10/15\n",
      "322560/322560 - 3s - loss: 0.1678 - accuracy: 0.9524 - val_loss: 0.1629 - val_accuracy: 0.9540\n",
      "Epoch 11/15\n",
      "322560/322560 - 3s - loss: 0.1627 - accuracy: 0.9535 - val_loss: 0.1651 - val_accuracy: 0.9638\n",
      "Epoch 12/15\n",
      "322560/322560 - 3s - loss: 0.1589 - accuracy: 0.9546 - val_loss: 0.1578 - val_accuracy: 0.9499\n",
      "Epoch 13/15\n",
      "322560/322560 - 3s - loss: 0.1557 - accuracy: 0.9554 - val_loss: 0.1521 - val_accuracy: 0.9581\n",
      "Epoch 14/15\n",
      "322560/322560 - 3s - loss: 0.1534 - accuracy: 0.9562 - val_loss: 0.1519 - val_accuracy: 0.9605\n",
      "Epoch 15/15\n",
      "322560/322560 - 3s - loss: 0.1514 - accuracy: 0.9569 - val_loss: 0.1485 - val_accuracy: 0.9599\n"
     ]
    }
   ],
   "source": [
    "model2_2 = build_model2(4,7)\n",
    "history2_2 = model2_2.fit(x_train2, y_train2, epochs=15, validation_split=0.2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520000/520000 [==============================] - 13s 25us/sample - loss: 0.3519 - accuracy: 0.8182\n",
      "[0.35193492987201763, 0.81815386]\n",
      "520000/520000 [==============================] - 10s 18us/sample - loss: 0.8955 - accuracy: 0.6702\n",
      "[0.895455308165917, 0.67021346]\n"
     ]
    }
   ],
   "source": [
    "print(model1.evaluate(x_test1, y_test1))\n",
    "print(model1_2.evaluate(x_test1, y_test1))\n",
    "# [0.09895923781607832, 0.85723215]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100800/100800 [==============================] - 3s 25us/sample - loss: 0.1059 - accuracy: 0.9703\n",
      "[0.10592036699639663, 0.9703274]\n",
      "100800/100800 [==============================] - 2s 19us/sample - loss: 0.1459 - accuracy: 0.9600\n",
      "[0.14590415938328655, 0.95996034]\n"
     ]
    }
   ],
   "source": [
    "print(model2.evaluate(x_test2, y_test2))\n",
    "print(model2_2.evaluate(x_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model successfully saved | file_location: C:\\Users\\archg\\school\\senoir\\xai-senior-design\\models\\model1.h5\n",
      "model successfully saved | file_location: C:\\Users\\archg\\school\\senoir\\xai-senior-design\\models\\model2.h5\n",
      "model successfully saved | file_location: C:\\Users\\archg\\school\\senoir\\xai-senior-design\\models\\model1_2.h5\n",
      "model successfully saved | file_location: C:\\Users\\archg\\school\\senoir\\xai-senior-design\\models\\model2_2.h5\n"
     ]
    }
   ],
   "source": [
    "#Only save the models if they need to be updated. Be sure to also save the new test-train data\n",
    "\n",
    "#save_model(model1, \"model1.h5\")\n",
    "#save_model(model2, \"model2.h5\")\n",
    "#save_model(model1_2, \"model1_2.h5\")\n",
    "#save_model(model2_2, \"model2_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
